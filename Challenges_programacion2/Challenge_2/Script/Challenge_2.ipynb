{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a62a9c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificando conexi√≥n con MLflow en http://127.0.0.1:5000...\n",
      "‚úì Conexi√≥n con MLflow verificada\n",
      "‚úì Experiment configurado: Books_Sentiment_Analysis\n",
      "\n",
      "=== Iniciando Pipeline ===\n",
      "\n",
      "Extrayendo datos de libros...\n",
      "Iniciando scraping desde: https://books.toscrape.com/\n",
      "Procesando p√°gina 1: https://books.toscrape.com/\n",
      "Procesando p√°gina 2: https://books.toscrape.com/catalogue/page-2.html\n",
      "Procesando p√°gina 3: https://books.toscrape.com/catalogue/page-3.html\n",
      "Procesando p√°gina 4: https://books.toscrape.com/catalogue/page-4.html\n",
      "Procesando p√°gina 5: https://books.toscrape.com/catalogue/page-5.html\n",
      "Procesando p√°gina 6: https://books.toscrape.com/catalogue/page-6.html\n",
      "Procesando p√°gina 7: https://books.toscrape.com/catalogue/page-7.html\n",
      "Procesando p√°gina 8: https://books.toscrape.com/catalogue/page-8.html\n",
      "Procesando p√°gina 9: https://books.toscrape.com/catalogue/page-9.html\n",
      "Procesando p√°gina 10: https://books.toscrape.com/catalogue/page-10.html\n",
      "Procesando p√°gina 11: https://books.toscrape.com/catalogue/page-11.html\n",
      "Procesando p√°gina 12: https://books.toscrape.com/catalogue/page-12.html\n",
      "Procesando p√°gina 13: https://books.toscrape.com/catalogue/page-13.html\n",
      "Procesando p√°gina 14: https://books.toscrape.com/catalogue/page-14.html\n",
      "Procesando p√°gina 15: https://books.toscrape.com/catalogue/page-15.html\n",
      "Procesando p√°gina 16: https://books.toscrape.com/catalogue/page-16.html\n",
      "Procesando p√°gina 17: https://books.toscrape.com/catalogue/page-17.html\n",
      "Procesando p√°gina 18: https://books.toscrape.com/catalogue/page-18.html\n",
      "Procesando p√°gina 19: https://books.toscrape.com/catalogue/page-19.html\n",
      "Procesando p√°gina 20: https://books.toscrape.com/catalogue/page-20.html\n",
      "Procesando p√°gina 21: https://books.toscrape.com/catalogue/page-21.html\n",
      "Procesando p√°gina 22: https://books.toscrape.com/catalogue/page-22.html\n",
      "Procesando p√°gina 23: https://books.toscrape.com/catalogue/page-23.html\n",
      "Procesando p√°gina 24: https://books.toscrape.com/catalogue/page-24.html\n",
      "Procesando p√°gina 25: https://books.toscrape.com/catalogue/page-25.html\n",
      "Procesando p√°gina 26: https://books.toscrape.com/catalogue/page-26.html\n",
      "Procesando p√°gina 27: https://books.toscrape.com/catalogue/page-27.html\n",
      "Procesando p√°gina 28: https://books.toscrape.com/catalogue/page-28.html\n",
      "Procesando p√°gina 29: https://books.toscrape.com/catalogue/page-29.html\n",
      "Procesando p√°gina 30: https://books.toscrape.com/catalogue/page-30.html\n",
      "Procesando p√°gina 31: https://books.toscrape.com/catalogue/page-31.html\n",
      "Procesando p√°gina 32: https://books.toscrape.com/catalogue/page-32.html\n",
      "Procesando p√°gina 33: https://books.toscrape.com/catalogue/page-33.html\n",
      "Procesando p√°gina 34: https://books.toscrape.com/catalogue/page-34.html\n",
      "Procesando p√°gina 35: https://books.toscrape.com/catalogue/page-35.html\n",
      "Procesando p√°gina 36: https://books.toscrape.com/catalogue/page-36.html\n",
      "Procesando p√°gina 37: https://books.toscrape.com/catalogue/page-37.html\n",
      "Procesando p√°gina 38: https://books.toscrape.com/catalogue/page-38.html\n",
      "Procesando p√°gina 39: https://books.toscrape.com/catalogue/page-39.html\n",
      "Procesando p√°gina 40: https://books.toscrape.com/catalogue/page-40.html\n",
      "Procesando p√°gina 41: https://books.toscrape.com/catalogue/page-41.html\n",
      "Procesando p√°gina 42: https://books.toscrape.com/catalogue/page-42.html\n",
      "Procesando p√°gina 43: https://books.toscrape.com/catalogue/page-43.html\n",
      "Procesando p√°gina 44: https://books.toscrape.com/catalogue/page-44.html\n",
      "Procesando p√°gina 45: https://books.toscrape.com/catalogue/page-45.html\n",
      "Procesando p√°gina 46: https://books.toscrape.com/catalogue/page-46.html\n",
      "Procesando p√°gina 47: https://books.toscrape.com/catalogue/page-47.html\n",
      "Procesando p√°gina 48: https://books.toscrape.com/catalogue/page-48.html\n",
      "Procesando p√°gina 49: https://books.toscrape.com/catalogue/page-49.html\n",
      "Procesando p√°gina 50: https://books.toscrape.com/catalogue/page-50.html\n",
      "No hay m√°s p√°ginas o no se encontr√≥ el bot√≥n 'siguiente'.\n",
      "Libros obtenidos: 1000\n",
      "\n",
      "Analizando sentimiento...\n",
      "\n",
      "Muestra de resultados:\n",
      "                                   title  sentiment\n",
      "0                   A Light in the Attic     0.9921\n",
      "1                     Tipping the Velvet     0.9769\n",
      "2                             Soumission     0.4215\n",
      "3                          Sharp Objects    -0.9873\n",
      "4  Sapiens: A Brief History of Humankind     0.9583\n",
      "\n",
      "=== Registrando en MLflow ===\n",
      "Sentimiento promedio: 0.39\n",
      "Datos registrados (o intento de registro) en MLflow.\n",
      "Run ID: a38fa153b5674ae4a0531d889da17130\n",
      "Para ver los resultados ejecuta: mlflow ui\n",
      "üèÉ View run dazzling-penguin-45 at: http://127.0.0.1:5000/#/experiments/456245077572084149/runs/a38fa153b5674ae4a0531d889da17130\n",
      "üß™ View experiment at: http://127.0.0.1:5000/#/experiments/456245077572084149\n",
      "\n",
      "=== Pipeline completado ===\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import socket\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def verify_local_server(uri):\n",
    "    \"\"\"Verifica que el URI apunte a un servidor local accesible\"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(uri)\n",
    "        if parsed.hostname not in ['localhost', '127.0.0.1']:\n",
    "            raise ValueError(f\"El host {parsed.hostname} no es una direcci√≥n local\")\n",
    "        \n",
    "        # Verifica si el puerto est√° disponible\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "            s.settimeout(2)\n",
    "            if s.connect_ex((parsed.hostname, parsed.port)) == 0:\n",
    "                return True\n",
    "            else:\n",
    "                raise ConnectionError(f\"No se puede conectar al servidor MLflow en {uri}\")\n",
    "    except Exception as e:\n",
    "        raise ConnectionError(f\"Error verificando servidor local: {str(e)}\")\n",
    "\n",
    "# ===== Configuraci√≥n Inicial con Verificaciones =====\n",
    "try:\n",
    "    # 1. Configuraci√≥n NLTK (con verificaci√≥n de recursos)\n",
    "    nltk_data_path = \"nltk_data\"\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "    \n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        nltk.data.find('corpora/stopwords')\n",
    "    except LookupError:\n",
    "        print(\"Descargando recursos NLTK... (esto puede tomar 1-2 minutos)\")\n",
    "        nltk.download('punkt', download_dir=nltk_data_path)\n",
    "        nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "        print(\"‚úì Recursos NLTK instalados\")\n",
    "    \n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # 2. Configuraci√≥n MLflow con verificaci√≥n expl√≠cita\n",
    "    mlflow_uri = \"http://127.0.0.1:5000\"\n",
    "    print(f\"\\nVerificando conexi√≥n con MLflow en {mlflow_uri}...\")\n",
    "    \n",
    "    # verify_local_server(mlflow_uri) # Comentado si causa problemas y el servidor est√° corriendo\n",
    "    mlflow.set_tracking_uri(mlflow_uri)\n",
    "    \n",
    "    # Verificaci√≥n adicional\n",
    "    if mlflow.get_tracking_uri() != mlflow_uri:\n",
    "        raise ValueError(f\"El URI de tracking no se estableci√≥ correctamente. Actual: {mlflow.get_tracking_uri()}\")\n",
    "    \n",
    "    print(\"‚úì Conexi√≥n con MLflow verificada\")\n",
    "    \n",
    "    # Crear/verificar experimento\n",
    "    experiment_name = \"Books_Sentiment_Analysis\"\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if not experiment:\n",
    "        print(f\"Creando nuevo experimento: {experiment_name}\")\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(f\"‚úì Experiment configurado: {experiment_name}\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n√ó Error en configuraci√≥n inicial: {str(e)}\")\n",
    "    print(\"Soluciones posibles:\")\n",
    "    print(\"1. Aseg√∫rate de tener el servidor MLflow ejecut√°ndose (ejecuta 'mlflow ui' en otra terminal)\")\n",
    "    print(\"2. Verifica que el puerto 5000 no est√© bloqueado por tu firewall\")\n",
    "    print(\"3. Revisa tu conexi√≥n a internet para descargar recursos NLTK\")\n",
    "    # exit(1) # Considera no salir si es un entorno interactivo\n",
    "\n",
    "# ===== 1. Web Scraping (Versi√≥n Corregida y Mejorada) =====\n",
    "def scrape_books(initial_url):\n",
    "    books = []\n",
    "    current_url = initial_url\n",
    "    page_count = 0\n",
    "    max_pages = 50 # El sitio books.toscrape.com tiene 50 p√°ginas de cat√°logo\n",
    "\n",
    "    print(f\"Iniciando scraping desde: {initial_url}\")\n",
    "\n",
    "    while current_url and page_count < max_pages:\n",
    "        page_count += 1\n",
    "        print(f\"Procesando p√°gina {page_count}: {current_url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(current_url, timeout=10)\n",
    "            response.raise_for_status() # Lanza una excepci√≥n para errores HTTP (4xx o 5xx)\n",
    "            \n",
    "            # Usar response.url como base para urljoin, ya que es la URL final despu√©s de posibles redirecciones.\n",
    "            actual_page_url = response.url \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            book_articles = soup.find_all('article', class_='product_pod')\n",
    "            if not book_articles:\n",
    "                print(f\"No se encontraron libros en {actual_page_url}. Deteniendo el scraping de esta rama.\")\n",
    "                break\n",
    "\n",
    "            for book_tag in book_articles:\n",
    "                title = book_tag.h3.a['title']\n",
    "                \n",
    "                relative_book_href = book_tag.h3.a['href']\n",
    "                detail_url = urljoin(actual_page_url, relative_book_href)\n",
    "                \n",
    "                desc = \"No description available\" \n",
    "                try:\n",
    "                    detail_response = requests.get(detail_url, timeout=5)\n",
    "                    detail_response.raise_for_status() \n",
    "                    detail_soup = BeautifulSoup(detail_response.text, 'html.parser')\n",
    "                    \n",
    "                    desc_tag = detail_soup.find('div', id='product_description')\n",
    "                    if desc_tag and desc_tag.find_next_sibling('p'):\n",
    "                        desc = desc_tag.find_next_sibling('p').text.strip()\n",
    "                    elif detail_soup.find('meta', attrs={'name': 'description'}):\n",
    "                        meta_desc = detail_soup.find('meta', attrs={'name': 'description'})\n",
    "                        if meta_desc and meta_desc.has_attr('content'):\n",
    "                            desc = meta_desc['content'].strip()\n",
    "                            \n",
    "                except requests.exceptions.HTTPError as http_err_detail:\n",
    "                    print(f\"Error HTTP obteniendo descripci√≥n para '{title}' de {detail_url}: {http_err_detail}\")\n",
    "                except requests.exceptions.RequestException as req_err_detail:\n",
    "                    print(f\"Error de Red obteniendo descripci√≥n para '{title}' de {detail_url}: {req_err_detail}\")\n",
    "                except Exception as e_detail:\n",
    "                    print(f\"Error gen√©rico obteniendo descripci√≥n para '{title}' ({detail_url}): {e_detail}\")\n",
    "                \n",
    "                books.append({'title': title, 'description': desc})\n",
    "                time.sleep(0.1) \n",
    "            \n",
    "            next_btn_tag = soup.find('li', class_='next')\n",
    "            if next_btn_tag and next_btn_tag.a and next_btn_tag.a.has_attr('href'):\n",
    "                relative_next_href = next_btn_tag.a['href']\n",
    "                current_url = urljoin(actual_page_url, relative_next_href)\n",
    "            else:\n",
    "                print(\"No hay m√°s p√°ginas o no se encontr√≥ el bot√≥n 'siguiente'.\")\n",
    "                current_url = None \n",
    "            \n",
    "            time.sleep(1) \n",
    "            \n",
    "        except requests.exceptions.HTTPError as http_err_main:\n",
    "            print(f\"Error HTTP al acceder a {current_url if current_url else 'URL desconocida'}: {http_err_main}\")\n",
    "            if http_err_main.response.status_code == 404:\n",
    "                 print(f\"Recibido 404 para {current_url if current_url else 'URL previa'}, el scraper se detendr√° para esta ruta.\")\n",
    "            current_url = None # Detener si hay un error 404 o similar en la p√°gina principal\n",
    "            break \n",
    "        except requests.exceptions.RequestException as req_err_main:\n",
    "            print(f\"Error de Red al acceder a {current_url if current_url else 'URL desconocida'}: {req_err_main}\")\n",
    "            current_url = None\n",
    "            break \n",
    "        except Exception as e_main:\n",
    "            print(f\"Error inesperado durante el scraping de {current_url if current_url else 'URL desconocida'}: {e_main}\")\n",
    "            current_url = None\n",
    "            break \n",
    "            \n",
    "    return pd.DataFrame(books)\n",
    "\n",
    "# ===== 2. Sentiment Analysis =====\n",
    "def analyze_sentiment(df):\n",
    "    # Verifica si la columna 'description' existe\n",
    "    if 'description' not in df.columns:\n",
    "        print(\"Advertencia: La columna 'description' no se encuentra en el DataFrame. El an√°lisis de sentimiento podr√≠a no funcionar como se espera.\")\n",
    "        df['sentiment'] = 0.0 # o alg√∫n valor por defecto\n",
    "        df['processed'] = \"\"\n",
    "        return df\n",
    "\n",
    "    def preprocess(text):\n",
    "        tokens = word_tokenize(str(text).lower()) # Convertir a string para evitar errores con None o float\n",
    "        stops = set(stopwords.words('english'))\n",
    "        return ' '.join([w for w in tokens if w.isalpha() and w not in stops])\n",
    "    \n",
    "    df['processed'] = df['description'].apply(preprocess)\n",
    "    df['sentiment'] = df['processed'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "    return df\n",
    "\n",
    "# ===== 3. MLflow Tracking Completo =====\n",
    "def log_to_mlflow(df):\n",
    "    # Iniciar experimento\n",
    "    # mlflow.set_experiment(\"Books_Sentiment_Analysis\") # Ya configurado globalmente\n",
    "\n",
    "    # Verificar si hay datos para registrar\n",
    "    if df.empty or 'sentiment' not in df.columns:\n",
    "        print(\"No hay datos analizados para registrar en MLflow o falta la columna 'sentiment'.\")\n",
    "        return\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        print(\"\\n=== Registrando en MLflow ===\")\n",
    "        \n",
    "        # 1. Par√°metros (inputs)\n",
    "        mlflow.log_param(\"source_url\", \"https://books.toscrape.com/\")\n",
    "        mlflow.log_param(\"num_books_processed\", len(df))\n",
    "        \n",
    "        # 2. M√©tricas (resultados num√©ricos)\n",
    "        avg_sentiment = df['sentiment'].mean()\n",
    "        mlflow.log_metric(\"avg_sentiment\", avg_sentiment)\n",
    "        print(f\"Sentimiento promedio: {avg_sentiment:.2f}\")\n",
    "        \n",
    "        # 3. Artefactos (archivos)\n",
    "        # Guardar datos\n",
    "        try:\n",
    "            df.to_csv(\"books_with_sentiment.csv\", index=False)\n",
    "            mlflow.log_artifact(\"books_with_sentiment.csv\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error guardando o registrando 'books_with_sentiment.csv': {e}\")\n",
    "\n",
    "        # Gr√°fica mejorada\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(df['sentiment'], bins=20, kde=True)\n",
    "            plt.title('Distribuci√≥n de Sentimiento en Descripciones de Libros')\n",
    "            plt.xlabel('Puntaje de Sentimiento')\n",
    "            plt.ylabel('N√∫mero de Libros')\n",
    "            plt.savefig(\"sentiment_distribution.png\")\n",
    "            mlflow.log_artifact(\"sentiment_distribution.png\")\n",
    "            plt.close() # Cerrar la figura para liberar memoria\n",
    "        except Exception as e:\n",
    "            print(f\"Error generando o registrando 'sentiment_distribution.png': {e}\")\n",
    "\n",
    "        # 4. Etiquetas\n",
    "        mlflow.set_tag(\"analysis_type\", \"sentiment_analysis\")\n",
    "        mlflow.set_tag(\"status\", \"completed\" if not df.empty else \"partial_data\")\n",
    "        \n",
    "        print(\"Datos registrados (o intento de registro) en MLflow.\")\n",
    "        print(f\"Run ID: {mlflow.active_run().info.run_id}\")\n",
    "        print(\"Para ver los resultados ejecuta: mlflow ui\")\n",
    "\n",
    "# ===== Ejecuci√≥n Principal =====\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Iniciando Pipeline ===\")\n",
    "    \n",
    "    # 1. Scraping\n",
    "    print(\"\\nExtrayendo datos de libros...\")\n",
    "    # Se usa la URL base, el scraper maneja la navegaci√≥n a partir de /catalogue/page-1.html\n",
    "    books_df = scrape_books(\"https://books.toscrape.com/\") \n",
    "    print(f\"Libros obtenidos: {len(books_df)}\")\n",
    "    \n",
    "    if not books_df.empty:\n",
    "        # 2. An√°lisis de sentimiento\n",
    "        print(\"\\nAnalizando sentimiento...\")\n",
    "        analyzed_df = analyze_sentiment(books_df.copy()) # Usar .copy() para evitar SettingWithCopyWarning\n",
    "        \n",
    "        # Mostrar algunos resultados\n",
    "        print(\"\\nMuestra de resultados:\")\n",
    "        if 'title' in analyzed_df.columns and 'sentiment' in analyzed_df.columns:\n",
    "            print(analyzed_df[['title', 'sentiment']].head())\n",
    "        else:\n",
    "            print(\"No se pudieron mostrar los resultados ya que faltan las columnas 'title' o 'sentiment'.\")\n",
    "\n",
    "        # 3. MLflow\n",
    "        try:\n",
    "            log_to_mlflow(analyzed_df)\n",
    "        except Exception as e_mlflow:\n",
    "            print(f\"Error durante el registro en MLflow: {e_mlflow}\")\n",
    "            print(\"Aseg√∫rate de que el servidor MLflow est√© activo en http://127.0.0.1:5000\")\n",
    "    else:\n",
    "        print(\"No se obtuvieron libros, se omiten los pasos de an√°lisis y registro en MLflow.\")\n",
    "    \n",
    "    print(\"\\n=== Pipeline completado ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_scraping_libros",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
